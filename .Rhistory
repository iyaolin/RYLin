skim_to_wide(dt)
library('RYLin')
loadPackages()
loadPackages(c('OpenML', 'mlr', 'h2o', 'caret'))
# global settings ---------------------------------------------------------
theme_set(theme_RYLin())
set.seed(787)
# load data ---------------------------------------------------------------
# OJ: Orange Juice Data
# https://rdrr.io/cran/ISLR/man/OJ.html
dt <- fread(
'data/orange_juice_withmissing.csv',
# na.strings = c("NA", "#DIV/0!", ""),
verbose = TRUE
)
# train[, lbl := 'train']
# test <- fread(
#   'data/pml-testing.csv',
#   na.strings = c("NA", "#DIV/0!", ""),
#   verbose = TRUE
# )
# test <- test[, 1:159, with = FALSE][, ':='(classe = NA_character_,
#                                            lbl = 'test')][]
#
# combine <- bind_rows(train, test)
# exploring the data ------------------------------------------------------
dim(dt)
str(dt)
# response variable
table(dt$Purchase)
# predictors --------------------------------------------------------------
# Descriptive statistics
skimr::skim_to_list(train)
# # correlation -------------------------------------------------------------
# library(corrplot)
# par(mar = c(4, 2, 2, 2))
# M <- cor(train[, -c('classe')])
# corrplot(
#   M,
#   tl.col = 'grey50',
#   type = "lower",
#   method = "square",
#   tl.cex = .6
# )
#
# # split train & test ------------------------------------------------------
# #Take training and break into validation and training.
# set.seed(787)
# inTrain <- createDataPartition(y = dt$classe, p = 0.6, list = FALSE)
# train <- dt[inTrain,]
# test <- dt[-inTrain,]
# preprocess --------------------------------------------------------------
# colnames ----------------------------------------------------------------
cols_all <- names(dt)
cols_y <- 'Purchase'
cols_lbl <- ''
cols_x <- setdiff(cols_all, c(cols_y, cols_lbl))
cols_cat <- c('Purchase', 'WeekofPurchase', 'StoreID', 'Store7', 'STORE', 'SpecialCH', 'SpecialMM')
cols_num <- setdiff(cols_x, cols_cat)
# valid_names <-
#   grep("^roll_|^pitch|^yaw_|^total_accel|^gyros_|^accel_|^magnet|classe",
#        names,
#        value = T)
# train <- train[, .SD, .SDcols = subsetnames]
# missing values ----------------------------------------------------------
#Identify the variables with majority NA, and remove
cols_na <- dt %>%
showNAInfo() %>%
'['(prop > 0.5, col)
if (length(cols_na) > 0L) {
dt[, (cols_na) := NULL]
}
# impute missing values using preProcess()
# Create the knn imputation model on the training data
missingvalue_model <- preProcess(dt, method = 'knnImpute')
missingvalue_model
# Use the imputation model to predict the values of missing data points
# library(RANN)  # required for knnInpute
dt <- predict(missingvalue_model, newdata = dt)
skim_to_wide(dt)
dt[, (cols_cat) := lapply(.SD, as.factor), .SDcols = cols_cat]
dt[, (cols_num) := lapply(.SD, as.numeric), .SDcols = cols_num]
skim_to_wide(dt)
table(dt$StoreID)
dt[, list(StoreID)]
dt
fillNAs
cols_cat <- c('Purchase', 'StoreID', 'Store7', 'STORE', 'SpecialCH', 'SpecialMM')
library('RYLin')
loadPackages()
loadPackages(c('OpenML', 'mlr', 'h2o', 'caret'))
# global settings ---------------------------------------------------------
theme_set(theme_RYLin())
set.seed(787)
# load data ---------------------------------------------------------------
# OJ: Orange Juice Data
# https://rdrr.io/cran/ISLR/man/OJ.html
dt <- fread(
'data/orange_juice_withmissing.csv',
# na.strings = c("NA", "#DIV/0!", ""),
verbose = TRUE
)
# train[, lbl := 'train']
# test <- fread(
#   'data/pml-testing.csv',
#   na.strings = c("NA", "#DIV/0!", ""),
#   verbose = TRUE
# )
# test <- test[, 1:159, with = FALSE][, ':='(classe = NA_character_,
#                                            lbl = 'test')][]
#
# combine <- bind_rows(train, test)
# exploring the data ------------------------------------------------------
dim(dt)
str(dt)
# response variable
table(dt$Purchase)
# predictors --------------------------------------------------------------
# Descriptive statistics
skimr::skim_to_list(train)
# # correlation -------------------------------------------------------------
# library(corrplot)
# par(mar = c(4, 2, 2, 2))
# M <- cor(train[, -c('classe')])
# corrplot(
#   M,
#   tl.col = 'grey50',
#   type = "lower",
#   method = "square",
#   tl.cex = .6
# )
#
# # split train & test ------------------------------------------------------
# #Take training and break into validation and training.
# set.seed(787)
# inTrain <- createDataPartition(y = dt$classe, p = 0.6, list = FALSE)
# train <- dt[inTrain,]
# test <- dt[-inTrain,]
# preprocess --------------------------------------------------------------
# colnames ----------------------------------------------------------------
cols_all <- names(dt)
cols_y <- 'Purchase'
cols_lbl <- ''
cols_x <- setdiff(cols_all, c(cols_y, cols_lbl))
cols_cat <- c('Purchase', 'StoreID', 'Store7', 'STORE', 'SpecialCH', 'SpecialMM')
cols_num <- setdiff(cols_x, cols_cat)
skim_to_wide(dt)
# valid_names <-
#   grep("^roll_|^pitch|^yaw_|^total_accel|^gyros_|^accel_|^magnet|classe",
#        names,
#        value = T)
# train <- train[, .SD, .SDcols = subsetnames]
dt[, (cols_cat) := lapply(.SD, as.factor), .SDcols = cols_cat]
dt[, (cols_num) := lapply(.SD, as.numeric), .SDcols = cols_num]
skim_to_wide(dt)
# missing values ----------------------------------------------------------
#Identify the variables with majority NA, and remove
cols_na <- dt %>%
showNAInfo() %>%
'['(prop > 0.5, col)
cols_ a
cols_na
if (length(cols_na) > 0L) {
dt[, (cols_na) := NULL]
}
# impute missing values using preProcess()
# Create the knn imputation model on the training data
missingvalue_model <- preProcess(dt, method = 'knnImpute')
missingvalue_model
# Use the imputation model to predict the values of missing data points
# library(RANN)  # required for knnInpute
dt <- predict(missingvalue_model, newdata = dt)
skim_to_wide(dt)
fillNAs(dt, 'StoreID')
skim_to_wide(dt)
fillNAs()
fillNAs
# categorical variables
for (col in cols_cat) {
fillNAs(dt, col)
}
dt
skim_to_wide(dt)
# Get rid of variables with near zero variance ----------------------------
nsv <- nearZeroVar(dt, saveMetrics = TRUE)
nsv
cols_nzv <- rownames(nsv[nsv$nzv == TRUE,])
cols_nzv
cols_nzv <- rownames(nsv[nsv$nzv == TRUE,])
if (length(cols_nzv) > 0L) {
dt[, (cols_na) := NULL]
}
dt
# feture engering ---------------------------------------------------------
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data = dt)
t <- predict(dummies_model, newdata = dt)
str(dt)
# Remove our result variable so it doesnâ€™t get preprocessed too -----------
cols_Y <- 'Purchase'
cols_X <- setdiff(names(dt), cols_Y)
trainX <- dt[, cols_X, with = FALSE]
trainY <- dt[, cols_Y, with = FALSE]
str(trainX)
# feture engering ---------------------------------------------------------
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data = trainX)
# feture engering ---------------------------------------------------------
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data = dt)
predict(dummies_model, newdata = dt)
predict(dummies_model, newdata = trainX)
# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
dt <- predict(dummies_model, newdata = dt)
str(dt)
library('RYLin')
loadPackages()
loadPackages(c('OpenML', 'mlr', 'h2o', 'caret'))
# global settings ---------------------------------------------------------
theme_set(theme_RYLin())
set.seed(787)
# load data ---------------------------------------------------------------
# OJ: Orange Juice Data
# https://rdrr.io/cran/ISLR/man/OJ.html
dt <- fread(
'data/orange_juice_withmissing.csv',
# na.strings = c("NA", "#DIV/0!", ""),
verbose = TRUE
)
# train[, lbl := 'train']
# test <- fread(
#   'data/pml-testing.csv',
#   na.strings = c("NA", "#DIV/0!", ""),
#   verbose = TRUE
# )
# test <- test[, 1:159, with = FALSE][, ':='(classe = NA_character_,
#                                            lbl = 'test')][]
#
# combine <- bind_rows(train, test)
# exploring the data ------------------------------------------------------
dim(dt)
str(dt)
# response variable
table(dt$Purchase)
# predictors --------------------------------------------------------------
# Descriptive statistics
skimr::skim_to_list(train)
# # correlation -------------------------------------------------------------
# library(corrplot)
# par(mar = c(4, 2, 2, 2))
# M <- cor(train[, -c('classe')])
# corrplot(
#   M,
#   tl.col = 'grey50',
#   type = "lower",
#   method = "square",
#   tl.cex = .6
# )
#
# # split train & test ------------------------------------------------------
# #Take training and break into validation and training.
# set.seed(787)
# inTrain <- createDataPartition(y = dt$classe, p = 0.6, list = FALSE)
# train <- dt[inTrain,]
# test <- dt[-inTrain,]
# preprocess --------------------------------------------------------------
# colnames ----------------------------------------------------------------
cols_all <- names(dt)
cols_y <- 'Purchase'
cols_lbl <- ''
cols_x <- setdiff(cols_all, c(cols_y, cols_lbl))
cols_cat <- c('Purchase', 'StoreID', 'Store7', 'STORE', 'SpecialCH', 'SpecialMM')
cols_num <- setdiff(cols_x, cols_cat)
# valid_names <-
#   grep("^roll_|^pitch|^yaw_|^total_accel|^gyros_|^accel_|^magnet|classe",
#        names,
#        value = T)
# train <- train[, .SD, .SDcols = subsetnames]
dt[, (cols_cat) := lapply(.SD, as.factor), .SDcols = cols_cat]
dt[, (cols_num) := lapply(.SD, as.numeric), .SDcols = cols_num]
skim_to_wide(dt)
# missing values ----------------------------------------------------------
#Identify the variables with majority NA, and remove
cols_na <- dt %>%
showNAInfo() %>%
'['(prop > 0.5, col)
if (length(cols_na) > 0L) {
dt[, (cols_na) := NULL]
}
# impute missing values using preProcess()
# Create the knn imputation model on the training data
missingvalue_model <- preProcess(dt, method = 'knnImpute')
missingvalue_model
# Use the imputation model to predict the values of missing data points
# library(RANN)  # required for knnInpute
dt <- predict(missingvalue_model, newdata = dt)
skim_to_wide(dt)
# categorical variables
for (col in cols_cat) {
fillNAs(dt, col)
}
skim_to_wide(dt)
# Get rid of variables with near zero variance ----------------------------
nsv <- nearZeroVar(dt[], saveMetrics = TRUE)
cols_nzv <- rownames(nsv[nsv$nzv == TRUE,])
if (length(cols_nzv) > 0L) {
dt[, (cols_na) := NULL]
}
# Remove the first six variables. They have very little value in prediction --
# cols_tbr <- cols_all[1:6]
# feture engering ---------------------------------------------------------
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data = dt)
# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
dt <- predict(dummies_model, newdata = dt)
# # Convert to dataframe
trainData <- data.table(trainData_mat)
# # Convert to dataframe
dt <- data.table(dt)
# # See the structure of the new dataset
skim_to_wide(dt)
# # See the structure of the new dataset
skim_to_list(dt)
dim(dt)
str(dt)
names(dt)
str(trainY)
library('RYLin')
loadPackages()
loadPackages(c('OpenML', 'mlr', 'h2o', 'caret'))
# global settings ---------------------------------------------------------
theme_set(theme_RYLin())
set.seed(787)
# load data ---------------------------------------------------------------
# OJ: Orange Juice Data
# https://rdrr.io/cran/ISLR/man/OJ.html
dt <- fread(
'data/orange_juice_withmissing.csv',
# na.strings = c("NA", "#DIV/0!", ""),
verbose = TRUE
)
# train[, lbl := 'train']
# test <- fread(
#   'data/pml-testing.csv',
#   na.strings = c("NA", "#DIV/0!", ""),
#   verbose = TRUE
# )
# test <- test[, 1:159, with = FALSE][, ':='(classe = NA_character_,
#                                            lbl = 'test')][]
#
# combine <- bind_rows(train, test)
# exploring the data ------------------------------------------------------
dim(dt)
str(dt)
# response variable
table(dt$Purchase)
# predictors --------------------------------------------------------------
# Descriptive statistics
skimr::skim_to_list(train)
# # correlation -------------------------------------------------------------
# library(corrplot)
# par(mar = c(4, 2, 2, 2))
# M <- cor(train[, -c('classe')])
# corrplot(
#   M,
#   tl.col = 'grey50',
#   type = "lower",
#   method = "square",
#   tl.cex = .6
# )
#
# # split train & test ------------------------------------------------------
# #Take training and break into validation and training.
# set.seed(787)
# inTrain <- createDataPartition(y = dt$classe, p = 0.6, list = FALSE)
# train <- dt[inTrain,]
# test <- dt[-inTrain,]
# preprocess --------------------------------------------------------------
# colnames ----------------------------------------------------------------
cols_all <- names(dt)
cols_y <- 'Purchase'
cols_lbl <- ''
cols_x <- setdiff(cols_all, c(cols_y, cols_lbl))
cols_cat <- c('Purchase', 'StoreID', 'Store7', 'STORE', 'SpecialCH', 'SpecialMM')
cols_num <- setdiff(cols_x, cols_cat)
# valid_names <-
#   grep("^roll_|^pitch|^yaw_|^total_accel|^gyros_|^accel_|^magnet|classe",
#        names,
#        value = T)
# train <- train[, .SD, .SDcols = subsetnames]
dt[, (cols_cat) := lapply(.SD, as.factor), .SDcols = cols_cat]
dt[, (cols_num) := lapply(.SD, as.numeric), .SDcols = cols_num]
skim_to_wide(dt)
# missing values ----------------------------------------------------------
#Identify the variables with majority NA, and remove
cols_na <- dt %>%
showNAInfo() %>%
'['(prop > 0.5, col)
if (length(cols_na) > 0L) {
dt[, (cols_na) := NULL]
}
# impute missing values using preProcess()
# Create the knn imputation model on the training data
missingvalue_model <- preProcess(dt, method = 'knnImpute')
missingvalue_model
# Use the imputation model to predict the values of missing data points
# library(RANN)  # required for knnInpute
dt <- predict(missingvalue_model, newdata = dt)
skim_to_wide(dt)
# categorical variables
for (col in cols_cat) {
fillNAs(dt, col)
}
skim_to_wide(dt)
# Get rid of variables with near zero variance ----------------------------
nsv <- nearZeroVar(dt[], saveMetrics = TRUE)
cols_nzv <- rownames(nsv[nsv$nzv == TRUE,])
if (length(cols_nzv) > 0L) {
dt[, (cols_na) := NULL]
}
# Remove the first six variables. They have very little value in prediction --
# cols_tbr <- cols_all[1:6]
# feture engering ---------------------------------------------------------
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data = dt)
# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
dt <- predict(dummies_model, newdata = dt)
# # Convert to dataframe
dt <- data.table(dt, Purchase = trainY$Purchase)
# # See the structure of the new dataset
skim_to_list(dt)
names(dt)
dt[, 1:18]
names(dt)
featurePlot(x = dt[, 1:18],
y = dt$Purchase,
plot = "box",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
dim(dt)
featurePlot(x = dt[, 1:32],
y = dt$Purchase,
plot = "box",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
# Get rid of variables with near zero variance ----------------------------
nsv <- nearZeroVar(dt[], saveMetrics = TRUE)
cols_nzv <- rownames(nsv[nsv$nzv == TRUE, ])
nzv
nsv
cols_nzv <- rownames(nsv[nsv$nzv == TRUE, ])
cols_nzv
modelLookup('earth')
modelLookup('svm')
modelLookup('xgboost')
modelLookup('knn')
library('RYLin')
loadPackages()
loadPackages(c('OpenML', 'mlr', 'h2o', 'caret', 'skimr'))
# global settings ---------------------------------------------------------
theme_set(theme_RYLin())
set.seed(787)
# load data ---------------------------------------------------------------
# Data: Orange Juice Data
# https://rdrr.io/cran/ISLR/man/OJ.html
dt <- fread('data/orange_juice_withmissing.csv',
# na.strings = c("NA", "#DIV/0!", ""),
verbose = TRUE)
library('RYLin')
loadPackages()
loadPackages(c('OpenML', 'mlr', 'h2o', 'caret', 'skimr'))
# global settings ---------------------------------------------------------
theme_set(theme_RYLin())
set.seed(787)
# load data ---------------------------------------------------------------
# Data: Orange Juice Data
# https://rdrr.io/cran/ISLR/man/OJ.html
dt <- fread('data/orange_juice_withmissing.csv',
# na.strings = c("NA", "#DIV/0!", ""),
verbose = TRUE)
# train[, lbl := 'train']
# test <- fread(
#   'data/pml-testing.csv',
#   na.strings = c("NA", "#DIV/0!", ""),
#   verbose = TRUE
# )
# test <- test[, 1:159, with = FALSE][, ':='(classe = NA_character_,
#                                            lbl = 'test')][]
#
# combine <- bind_rows(train, test)
# exploring the data ------------------------------------------------------
dim(dt)
str(dt)
# response variable
table(dt$Purchase)
# response variable
describe(dt$Purchase)
# descriptive statistics
skimr::skim_to_list(train)
# viewSingleVar: table view of the distribution of variable x
viewSingleVar <- function(dt, col){
dt[, .N, keyby = base::get(col)][, pct := sprintf("%.3f", N / sum(N))][]
}
updateRYLin()
library(RYLin)
