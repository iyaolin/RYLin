#   destination_frames = c("train.hex", "valid.hex", "test.hex"), seed = 787
# )
# train <- splits[[1]]
# valid <- splits[[2]]
# test  <- splits[[3]]
splits <- h2o.splitFrame(
data = df,
ratios = c(0.8),   ## only need to specify 2 fractions, the 3rd is implied
destination_frames = c("train.hex", "valid.hex"), seed = 787
)
trainData <- splits[[1]]
validData <- splits[[2]]
hyper_params = list(
## restrict the search to the range of max_depth established above
max_depth = seq(3, 9, 2),
## search a large space of row sampling rates per tree
# sample_rate = seq(0.2, 1, 0.1),
## search a large space of column sampling rates per split
# col_sample_rate = seq(0.2, 1, 0.1),
## search a large space of column sampling rates per tree
# col_sample_rate_per_tree = seq(0.2, 1, 0.1),
## search a large space of how column sampling per split should change as a function of the depth of the split
# col_sample_rate_change_per_level = seq(0.9, 1.1, 0.01),
## search a large space of the number of min rows in a terminal node
# min_rows = 2 ^ seq(0, log2(nrow(trainData)) - 1, 1),
## search a large space of the number of bins for split-finding for continuous and integer columns
# nbins = 2 ^ seq(4, 10, 1),
## search a large space of the number of bins for split-finding for categorical columns
# nbins_cats = 2 ^ seq(4, 12, 1),
## search a few minimum required relative error improvement thresholds for a split to happen
# min_split_improvement = c(0, 1e-8, 1e-6, 1e-4),
min_split_improvement = c(1e-4),
## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
histogram_type = c("UniformAdaptive", "QuantilesGlobal", "RoundRobin")
)
hyper_params = list(
## restrict the search to the range of max_depth established above
max_depth = seq(3, 9, 2),
## search a large space of row sampling rates per tree
# sample_rate = seq(0.2, 1, 0.1),
## search a large space of column sampling rates per split
# col_sample_rate = seq(0.2, 1, 0.1),
## search a large space of column sampling rates per tree
# col_sample_rate_per_tree = seq(0.2, 1, 0.1),
## search a large space of how column sampling per split should change as a function of the depth of the split
# col_sample_rate_change_per_level = seq(0.9, 1.1, 0.01),
## search a large space of the number of min rows in a terminal node
# min_rows = 2 ^ seq(0, log2(nrow(trainData)) - 1, 1),
## search a large space of the number of bins for split-finding for continuous and integer columns
# nbins = 2 ^ seq(4, 10, 1),
## search a large space of the number of bins for split-finding for categorical columns
# nbins_cats = 2 ^ seq(4, 12, 1),
## search a few minimum required relative error improvement thresholds for a split to happen
# min_split_improvement = c(0, 1e-8, 1e-6, 1e-4),
min_split_improvement = c(1e-4)
## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
# histogram_type = c("UniformAdaptive", "QuantilesGlobal", "RoundRobin")
)
search_criteria = list(
## Random grid search
strategy = "RandomDiscrete"
#
# ## limit the runtime to 60 minutes
# max_runtime_secs = 3600,
#
# ## build no more than 100 models
# max_models = 100,
# ## random number generator seed to make sampling of parameter combinations reproducible
# seed = 1234,
#
# ## early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference
# stopping_rounds = 5,
# stopping_metric = "AUC",
# stopping_tolerance = 1e-3
)
grid <- h2o.grid(
## hyper parameters
hyper_params = hyper_params,
## hyper-parameter search configuration (see above)
search_criteria = search_criteria,
## which algorithm to run
algorithm = "gbm",
## identifier for the grid, to later retrieve it
grid_id = "final_grid",
## standard model parameters
x = cols_x,
y = cols_y,
training_frame = trainData,
validation_frame = validData,
## more trees is better if the learning rate is small enough
## use "more than enough" trees - we have early stopping
ntrees = 10000,
## smaller learning rate is better
## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
## (use 1.00 to disable, but then lower the learning_rate)
learn_rate_annealing = 0.99,
## early stopping based on timeout (no model should take more than 1 hour - modify as needed)
max_runtime_secs = 3600,
## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
stopping_rounds = 5,
stopping_tolerance = 1e-4,
stopping_metric = "AUC",
## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
score_tree_interval = 10,
## base random number generator seed for each model (automatically gets incremented internally for each model)
seed = 1234
)
grid
## Sort the grid models by AUC
sortedGrid <-
h2o.getGrid("final_grid", sort_by = "auc", decreasing = TRUE)
sortedGrid
seq(5, 5, 1)
sortedGrid <-
h2o.getGrid("final_grid", sort_by = "auc", decreasing = TRUE)
sortedGrid
hyper_params = list(
## restrict the search to the range of max_depth established above
max_depth = seq(5, 5, 1),
## search a large space of row sampling rates per tree
sample_rate = seq(0.2, 1, 0.1),
## search a large space of column sampling rates per split
# col_sample_rate = seq(0.2, 1, 0.1),
## search a large space of column sampling rates per tree
# col_sample_rate_per_tree = seq(0.2, 1, 0.1),
## search a large space of how column sampling per split should change as a function of the depth of the split
# col_sample_rate_change_per_level = seq(0.9, 1.1, 0.01),
## search a large space of the number of min rows in a terminal node
# min_rows = 2 ^ seq(0, log2(nrow(trainData)) - 1, 1),
## search a large space of the number of bins for split-finding for continuous and integer columns
# nbins = 2 ^ seq(4, 10, 1),
## search a large space of the number of bins for split-finding for categorical columns
# nbins_cats = 2 ^ seq(4, 12, 1),
## search a few minimum required relative error improvement thresholds for a split to happen
# min_split_improvement = c(0, 1e-8, 1e-6, 1e-4),
min_split_improvement = c(1e-4)
## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
# histogram_type = c("UniformAdaptive", "QuantilesGlobal", "RoundRobin")
)
search_criteria = list(
## Random grid search
strategy = "RandomDiscrete",
## limit the runtime to 30 minutes
max_runtime_secs = 1800,
## build no more than 100 models
max_models = 100,
## early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference
stopping_rounds = 5,
stopping_tolerance = 1e-3,
stopping_metric = "AUC",
## random number generator seed to make sampling of parameter combinations reproducible
seed = 1234
)
grid <- h2o.grid(
## hyper parameters
hyper_params = hyper_params,
## hyper-parameter search configuration (see above)
search_criteria = search_criteria,
## which algorithm to run
algorithm = "gbm",
## identifier for the grid, to later retrieve it
grid_id = "final_grid",
## standard model parameters
x = cols_x,
y = cols_y,
training_frame = trainData,
validation_frame = validData,
## more trees is better if the learning rate is small enough
## use "more than enough" trees - we have early stopping
ntrees = 10000,
## smaller learning rate is better
## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
## (use 1.00 to disable, but then lower the learning_rate)
learn_rate_annealing = 0.99,
## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
score_tree_interval = 10,
## base random number generator seed for each model (automatically gets incremented internally for each model)
seed = 787
)
## Sort the grid models by AUC
sortedGrid <-
h2o.getGrid("final_grid", sort_by = "auc", decreasing = TRUE)
sortedGrid
hyper_params
rm(grid)
rm(final_grid)
rm(sortedGrid)
hyper_params = list(
## restrict the search to the range of max_depth established above
max_depth = seq(5, 5, 1),
## search a large space of row sampling rates per tree
sample_rate = seq(0.2, 1, 0.1),
## search a large space of column sampling rates per split
# col_sample_rate = seq(0.2, 1, 0.1),
## search a large space of column sampling rates per tree
# col_sample_rate_per_tree = seq(0.2, 1, 0.1),
## search a large space of how column sampling per split should change as a function of the depth of the split
# col_sample_rate_change_per_level = seq(0.9, 1.1, 0.01),
## search a large space of the number of min rows in a terminal node
# min_rows = 2 ^ seq(0, log2(nrow(trainData)) - 1, 1),
## search a large space of the number of bins for split-finding for continuous and integer columns
# nbins = 2 ^ seq(4, 10, 1),
## search a large space of the number of bins for split-finding for categorical columns
# nbins_cats = 2 ^ seq(4, 12, 1),
## search a few minimum required relative error improvement thresholds for a split to happen
# min_split_improvement = c(0, 1e-8, 1e-6, 1e-4),
min_split_improvement = c(1e-4)
## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
# histogram_type = c("UniformAdaptive", "QuantilesGlobal", "RoundRobin")
)
search_criteria = list(
## Random grid search
strategy = "RandomDiscrete",
## limit the runtime to 30 minutes
max_runtime_secs = 1800,
## build no more than 100 models
max_models = 100,
## early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference
stopping_rounds = 5,
stopping_tolerance = 1e-3,
stopping_metric = "AUC",
## random number generator seed to make sampling of parameter combinations reproducible
seed = 1234
)
grid <- h2o.grid(
## hyper parameters
hyper_params = hyper_params,
## hyper-parameter search configuration (see above)
search_criteria = search_criteria,
## which algorithm to run
algorithm = "gbm",
## identifier for the grid, to later retrieve it
grid_id = "finalGrid",
## standard model parameters
x = cols_x,
y = cols_y,
training_frame = trainData,
validation_frame = validData,
## more trees is better if the learning rate is small enough
## use "more than enough" trees - we have early stopping
ntrees = 10000,
## smaller learning rate is better
## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
## (use 1.00 to disable, but then lower the learning_rate)
learn_rate_annealing = 0.99,
## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
score_tree_interval = 10,
## base random number generator seed for each model (automatically gets incremented internally for each model)
seed = 787
)
## Sort the grid models by AUC
sortedGrid <-
h2o.getGrid("finalGrid", sort_by = "auc", decreasing = TRUE)
sortedGrid
seq(0.7, 0.9, 0.2)
2 ^ seq(4, 10, 1)
2 ^ seq(3, 5, 1)
hyper_params = list(
## restrict the search to the range of max_depth established above
max_depth = seq(5, 5, 1),
## search a large space of row sampling rates per tree
sample_rate = seq(0.9),
## search a large space of column sampling rates per split
col_sample_rate = seq(0.7, 0.9, 0.2),
## search a large space of column sampling rates per tree
# col_sample_rate_per_tree = seq(0.2, 1, 0.1),
## search a large space of how column sampling per split should change as a function of the depth of the split
# col_sample_rate_change_per_level = seq(0.9, 1.1, 0.01),
## search a large space of the number of min rows in a terminal node
# min_rows = 2 ^ seq(0, log2(nrow(trainData)) - 1, 1),
## search a large space of the number of bins for split-finding for continuous and integer columns
nbins = 2 ^ seq(3, 7, 2),
## search a large space of the number of bins for split-finding for categorical columns
# nbins_cats = 2 ^ seq(4, 12, 1),
## search a few minimum required relative error improvement thresholds for a split to happen
# min_split_improvement = c(0, 1e-8, 1e-6, 1e-4),
min_split_improvement = c(1e-4)
## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
# histogram_type = c("UniformAdaptive", "QuantilesGlobal", "RoundRobin")
)
search_criteria = list(
## Random grid search
strategy = "RandomDiscrete",
## limit the runtime to 30 minutes
max_runtime_secs = 1800,
## build no more than 100 models
max_models = 100,
## early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference
stopping_rounds = 5,
stopping_tolerance = 1e-3,
stopping_metric = "AUC",
## random number generator seed to make sampling of parameter combinations reproducible
seed = 1234
)
grid <- h2o.grid(
## hyper parameters
hyper_params = hyper_params,
## hyper-parameter search configuration (see above)
search_criteria = search_criteria,
## which algorithm to run
algorithm = "gbm",
## identifier for the grid, to later retrieve it
grid_id = "finalGrid",
## standard model parameters
x = cols_x,
y = cols_y,
training_frame = trainData,
validation_frame = validData,
## more trees is better if the learning rate is small enough
## use "more than enough" trees - we have early stopping
ntrees = 1000,
## smaller learning rate is better
## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
## (use 1.00 to disable, but then lower the learning_rate)
learn_rate_annealing = 0.99,
## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
score_tree_interval = 10,
## base random number generator seed for each model (automatically gets incremented internally for each model)
seed = 787
)
## Sort the grid models by AUC
sortedGrid <-
h2o.getGrid("finalGrid", sort_by = "auc", decreasing = TRUE)
sortedGrid
hyper_params = list(
## restrict the search to the range of max_depth established above
max_depth = seq(5, 5, 1),
## search a large space of row sampling rates per tree
sample_rate = seq(0.9),
## search a large space of column sampling rates per split
col_sample_rate = seq(0.7, 0.9, 0.2),
## search a large space of column sampling rates per tree
# col_sample_rate_per_tree = seq(0.2, 1, 0.1),
## search a large space of how column sampling per split should change as a function of the depth of the split
# col_sample_rate_change_per_level = seq(0.9, 1.1, 0.01),
## search a large space of the number of min rows in a terminal node
# min_rows = 2 ^ seq(0, log2(nrow(trainData)) - 1, 1),
## search a large space of the number of bins for split-finding for continuous and integer columns
nbins = 2 ^ seq(3, 7, 2),
## search a large space of the number of bins for split-finding for categorical columns
# nbins_cats = 2 ^ seq(4, 12, 1),
## search a few minimum required relative error improvement thresholds for a split to happen
# min_split_improvement = c(0, 1e-8, 1e-6, 1e-4),
min_split_improvement = c(1e-4)
## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
# histogram_type = c("UniformAdaptive", "QuantilesGlobal", "RoundRobin")
)
search_criteria = list(
## Random grid search
strategy = "RandomDiscrete",
## limit the runtime to 30 minutes
max_runtime_secs = 1800,
## build no more than 100 models
max_models = 100,
## early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference
stopping_rounds = 5,
stopping_tolerance = 1e-3,
stopping_metric = "AUC",
## random number generator seed to make sampling of parameter combinations reproducible
seed = 1234
)
grid <- h2o.grid(
## hyper parameters
hyper_params = hyper_params,
## hyper-parameter search configuration (see above)
search_criteria = search_criteria,
## which algorithm to run
algorithm = "gbm",
## identifier for the grid, to later retrieve it
grid_id = "finalGrid",
## standard model parameters
x = cols_x,
y = cols_y,
training_frame = trainData,
validation_frame = validData,
## more trees is better if the learning rate is small enough
## use "more than enough" trees - we have early stopping
ntrees = 1000,
## smaller learning rate is better
## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
## (use 1.00 to disable, but then lower the learning_rate)
learn_rate_annealing = 0.99,
## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
score_tree_interval = 10,
## base random number generator seed for each model (automatically gets incremented internally for each model)
seed = 787
)
## Sort the grid models by AUC
sortedGrid <-
h2o.getGrid("finalGrid", sort_by = "auc", decreasing = TRUE)
sortedGrid
## Sort the grid models by AUC
sortedGrid <-
h2o.getGrid("finalGrid", sort_by = "auc", decreasing = TRUE)
sortedGrid
hyper_params = list(
## restrict the search to the range of max_depth established above
max_depth = seq(5, 5, 1),
## search a large space of row sampling rates per tree
sample_rate = seq(0.9),
## search a large space of column sampling rates per split
col_sample_rate = seq(0.7, 0.9, 0.2),
## search a large space of column sampling rates per tree
# col_sample_rate_per_tree = seq(0.2, 1, 0.1),
## search a large space of how column sampling per split should change as a function of the depth of the split
# col_sample_rate_change_per_level = seq(0.9, 1.1, 0.01),
## search a large space of the number of min rows in a terminal node
# min_rows = 2 ^ seq(0, log2(nrow(trainData)) - 1, 1),
## search a large space of the number of bins for split-finding for continuous and integer columns
nbins = 2 ^ seq(3, 7, 2),
## search a large space of the number of bins for split-finding for categorical columns
# nbins_cats = 2 ^ seq(4, 12, 1),
## search a few minimum required relative error improvement thresholds for a split to happen
# min_split_improvement = c(0, 1e-8, 1e-6, 1e-4),
min_split_improvement = c(1e-4)
## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
# histogram_type = c("UniformAdaptive", "QuantilesGlobal", "RoundRobin")
)
search_criteria = list(
## Random grid search
strategy = "RandomDiscrete",
## limit the runtime to 30 minutes
max_runtime_secs = 1800,
## build no more than 100 models
max_models = 100,
## early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference
stopping_rounds = 5,
stopping_tolerance = 1e-3,
stopping_metric = "AUC",
## random number generator seed to make sampling of parameter combinations reproducible
seed = 1234
)
grid <- h2o.grid(
## hyper parameters
hyper_params = hyper_params,
## hyper-parameter search configuration (see above)
search_criteria = search_criteria,
## which algorithm to run
algorithm = "gbm",
## identifier for the grid, to later retrieve it
grid_id = "finalGrid",
## standard model parameters
x = cols_x,
y = cols_y,
training_frame = trainData,
validation_frame = validData,
## more trees is better if the learning rate is small enough
## use "more than enough" trees - we have early stopping
ntrees = 1000,
## smaller learning rate is better
## since we have learning_rate_annealing, we can afford to start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
## (use 1.00 to disable, but then lower the learning_rate)
learn_rate_annealing = 0.99,
## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
score_tree_interval = 10,
## base random number generator seed for each model (automatically gets incremented internally for each model)
seed = 787
)
## Sort the grid models by AUC
sortedGrid <-
h2o.getGrid("finalGrid", sort_by = "auc", decreasing = TRUE)
sortedGrid
hyper_params = list(
## restrict the search to the range of max_depth established above
max_depth = seq(5, 5, 1),
## search a large space of row sampling rates per tree
sample_rate = seq(0.9),
## search a large space of column sampling rates per split
col_sample_rate = seq(0.7, 0.9, 0.2),
## search a large space of column sampling rates per tree
# col_sample_rate_per_tree = seq(0.2, 1, 0.1),
## search a large space of how column sampling per split should change as a function of the depth of the split
# col_sample_rate_change_per_level = seq(0.9, 1.1, 0.01),
## search a large space of the number of min rows in a terminal node
# min_rows = 2 ^ seq(0, log2(nrow(trainData)) - 1, 1),
## search a large space of the number of bins for split-finding for continuous and integer columns
nbins = 2 ^ seq(3, 7, 2),
## search a large space of the number of bins for split-finding for categorical columns
# nbins_cats = 2 ^ seq(4, 12, 1),
## search a few minimum required relative error improvement thresholds for a split to happen
# min_split_improvement = c(0, 1e-8, 1e-6, 1e-4),
min_split_improvement = c(1e-4)
## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
# histogram_type = c("UniformAdaptive", "QuantilesGlobal", "RoundRobin")
)
search_criteria = list(
## Random grid search
strategy = "RandomDiscrete",
## limit the runtime to 30 minutes
max_runtime_secs = 1800,
## build no more than 100 models
max_models = 100,
## early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference
stopping_rounds = 5,
stopping_tolerance = 1e-3,
stopping_metric = "AUC",
## random number generator seed to make sampling of parameter combinations reproducible
seed = 1234
)
hyper_params
